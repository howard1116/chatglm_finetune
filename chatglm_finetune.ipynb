{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utDw6VXqSnhb"
   },
   "outputs": [],
   "source": [
    "conda create -n llm_env python=3.8\n",
    "conda activate llm_env\n",
    "pip install -r requirements.txt\n",
    "cd peft-chatglm\n",
    "python setup.py install\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b\n",
    "cd chatglm-6b\n",
    "# https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdUiLgxMRsb2"
   },
   "outputs": [],
   "source": [
    "# LoRA Finetune\n",
    "python train.py \\\n",
    "    --train_path data/mixed_train_dataset.jsonl \\\n",
    "    --dev_path data/mixed_dev_dataset.jsonl \\\n",
    "    --use_lora True \\\n",
    "    --lora_rank 8 \\\n",
    "    --batch_size 1 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --save_freq 1000 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --logging_steps 100 \\\n",
    "    --max_source_seq_len 300 \\\n",
    "    --max_target_seq_len 200 \\\n",
    "    --save_dir checkpoints/finetune \\\n",
    "    --img_log_dir \"log\" \\\n",
    "    --img_log_name \"ChatGLM Fine-Tune\" \\\n",
    "    --device cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 883645,
     "status": "ok",
     "timestamp": 1706787335429,
     "user": {
      "displayName": "howard brown",
      "userId": "08057938432834579792"
     },
     "user_tz": -480
    },
    "id": "GuznM9H4Rsb3",
    "outputId": "937bfec3-7c05-4752-d06a-69c45b09c7fc"
   },
   "outputs": [],
   "source": [
    "# P-Tuning\n",
    "python train.py \\\n",
    "    --train_path data/mixed_train_dataset.jsonl \\\n",
    "    --dev_path data/mixed_dev_dataset.jsonl \\\n",
    "    --use_ptuning True \\\n",
    "    --pre_seq_len 128 \\\n",
    "    --batch_size 1 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --save_freq 900 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --logging_steps 100 \\\n",
    "    --max_source_seq_len 300 \\\n",
    "    --max_target_seq_len 200 \\\n",
    "    --save_dir checkpoints/ptuning \\\n",
    "    --img_log_dir \"log\" \\\n",
    "    --img_log_name \"ChatGLM P-Tuning\" \\\n",
    "    --device cuda:0"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
